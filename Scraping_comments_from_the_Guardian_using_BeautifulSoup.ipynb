{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping comments from the Guardian using BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial I will demonstrate how BeautifulSoup from the Python library bs4 can be used to scrape comments from news websites, using the Guardian as an example. The code that is presented is largely derived from [tfeltwells' repository](https://github.com/tfeltwell/Guardian-comment-scraper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install bs4 and urllib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is install the two libraries we are going to use, `bs4` and `urllib`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install bs4\n",
    "pip install urllib\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an article from the Guardian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an article you wish to scrape the comments from. Note that if the article has no comments, the program will return an attribute error. More on this later...\n",
    "\n",
    "For this tutorial we will use the following url:\n",
    "\n",
    "```python\n",
    "url = 'https://www.theguardian.com/music/2019/feb/21/peter-tork-monkees-dies-aged-77'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the article in HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup`, a function in the `bs4` library, reads HTML. In order to convert our url to HTML we can use `urllib`.\n",
    "\n",
    "Start by importing `BeautifulSoup` and `urllib` and converting the url to HTML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "\n",
    "html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `soup` represents the webpage as a HTML document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick start HTML guide "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HTML is a markup language for creating web pages and web applications\n",
    "- Structurally it contains nested HTML **elements**\n",
    "- The elements are indicated in the document by **tags**\n",
    "- Tags are enclosed in angled brakcets, eg. `<p>` is the start tag for a paragraph and `</p>` is the end tag\n",
    "- A useful list of element tags can be found [here](https://www.w3schools.com/tags/)\n",
    "\n",
    "As an example, a basic HTML document may look like:\n",
    "\n",
    "```html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title> This is a title </title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p name = \"paragraph-1\"> My first paragraph </p>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "```\n",
    "\n",
    "In the above, four different tags are used: `<html>`, `<head>`, `<title>`, `<body>` and `<p>`. Each tag has a corresponding end tag where the name of the tag is preceeded by a `/`. \n",
    "\n",
    "Looking at the paragraph element: \n",
    "\n",
    "```html \n",
    "<p name = \"paragraph-1\"> My first paragraph </p> \n",
    "```\n",
    "you see that the element has been given an **attribute**. The attribute is `name` and its value is `paragraph-1`. \n",
    "\n",
    "Attributes are useful for searching HTML documents. As a document can have many instances of the same tag (eg. contain many paragraphs), if each tag has unique attributes then by searching for a tag and attribute pair the correct element will be returned.\n",
    "\n",
    "For more on HTML see the [Wikipedia]() page or have a go at [codecademy's](https://www.codecademy.com) course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a HTML document for our url contained in the object `soup`. What we want to do is search the document for the comments and other relevant information.\n",
    "\n",
    "At this point it is useful to open the `url` in your browser and view the HTML directly. This can be done by entering developer mode or, in Safari, by displaying the web inspector (Develop -> Show Web Inspector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by finding the title of the article. The title is a *level one heading* and will therefore have a `h1` tag. By exploring the HTML in your browser, you will find that the `h1` tag also has attributes. Here we will use the `class` attribute to find the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "article_title = soup.find('h1', class_='content__headline')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at `article_title`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h1 class=\"content__headline \" itemprop=\"headline\">\n",
    "Peter Tork, bassist for the Monkees, dies aged 77\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that the `find()` function has returned the entire HTML element. As we only want the title text, we can use the `getText()`, `strip()` and `encode()` functions to convert the title to a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "article_title = article_title.getText().strip().encode('utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to find the url that links to the comments section of the article. As we are looking for a hyperlink, the element needed will have an `a` tag and an attribute `href` corresponding to the url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "comment_url = soup.find(class_='discussion__heading').find('a')['href']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is first searching the HTML document for an element whose `class` attribute is `discussion__heading`. It then searches within that element for an element with an `a` tag. Finally it selects the value of the attribute `href` within this element. The corresponding HTML structure is:\n",
    "\n",
    "```html\n",
    "\n",
    "<div class=\"discussion__heading\">\n",
    "    <div class=\"container__meta modern-hidden\">\n",
    "        <h2 class=\"container__meta__title\">\n",
    "            <a href=\"https://www.theguardian.com/discussion/p/az4e5\" data-link-name=\"View all comments\">\n",
    "            View all comments &gt;</a>\n",
    "        </h2>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "```\n",
    "\n",
    "From this we can see that `comment_url` now has the value `https://www.theguardian.com/discussion/p/az4e5` which is a webpage containing only the comments from the article.\n",
    "\n",
    "If it is not possible to comment on the article, as is the case with some articles on the Guardian, you will get an error message at this point. This is because the class `discussion__heading` doesn't exist and therefore it is not possible to proceed with scraping comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to get the HTML document for the comments using, again, urllib and BeautifulSoup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "html = urllib.urlopen(comment_url).read()\n",
    "comment_soup = BeautifulSoup(html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `comment_soup` is now a HTML document containing the details of all comments on the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developer mode you will find that the comments are stored as a list; each comment has its own element with a `li` tag. In order to scrape the comments and any additional information that might be needed we want to iterate over this list, storing the data for each comment in `comment_array`. \n",
    "\n",
    "To start, we will find the ID and time stamp of the comment along with the author's name and ID and create a dictionary to store the information: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "comment_array=[]\n",
    "\n",
    "for comment in comment_soup.select('li.d-comment'):\n",
    "    \n",
    "    commentObj = {}\n",
    "    \n",
    "    commentObj['id'] = comment['data-comment-id']\n",
    "    commentObj['timestamp'] = comment['data-comment-timestamp']\n",
    "    commentObj['author'] = comment['data-comment-author'].encode('utf-8')\n",
    "    commentObj['author-id'] = comment['data-comment-author-id']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all of this information is contained in `attributes` of the `li` tag it can be accessed directly from each `comment` we iterate over. The `attributes` of each `li` tag can be accessed by treating the tag like a python dictionary. You can see that the `attributes` are `keys` to a dictionary and can be used to get the `values` of each `attribute`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the text within each `li` tag corresponding to the coment text, we search for the element using the class attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    body = comment.find(class_='d-comment__body')\n",
    "\n",
    "    if body.blockquote is not None:\n",
    "        body.blockquote.clear()\n",
    "\n",
    "    commentObj['text'] = body.getText().strip().encode('utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `if` statement is needed to account for comments that have been blocked: while the content of blocked comments cannot be viewed on the webpage the data is still contained in the HTML document. Similarly to obtaining the title text, we need to use the `getText()`, `strip()` and `encode()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful attributes include the number of people that have recommended or agreed with the comment and, if the comment is a reply to someone else's, the ID of the original comment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    recommend = comment.find(class_='d-comment__recommend')\n",
    "    if recommend is not None:\n",
    "        commentObj['reccomend-count'] = recommend['data-recommend-count']\n",
    "    else:\n",
    "        commentObj['reccomend-count'] = ''\n",
    "            \n",
    "    replyto = comment.find(class_='d-comment__reply-to-author')\n",
    "\n",
    "    if replyto is not None:\n",
    "        link = replyto.parent['href'].replace('#comment-', '')\n",
    "        commentObj['reply-to'] = link\n",
    "    else:\n",
    "        commentObj['reply-to'] = ''\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have used `if` statements to account for comments that are not replies and comments that cannot be recommended.\n",
    "\n",
    "Finally, we should add our data for this comment to `comment_array` before moving on to the next comment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    comment_array.append(commentObj)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a script that will extract the comments from your chosen url and save them to a `json` file. It includes functions for:\n",
    "- getting the HTML document, `get_html()` \n",
    "- saving the comments, `save()`\n",
    "- scraping the comments, `get_comments()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup\n",
    "\n",
    "def save(fname,data):\n",
    "    fileName = fname+'.json'\n",
    "    outFile = open(fileName,'w')\n",
    "    json.dump(data, outFile)\n",
    "\n",
    "def get_comments(url):\n",
    "    \n",
    "    soup = get_html(url)\n",
    "\n",
    "    comment_array=[]\n",
    "\n",
    "    for comment in soup.select('li.d-comment'):\n",
    "\n",
    "        commentObj = {}\n",
    "\n",
    "        commentObj['id'] = comment['data-comment-id']\n",
    "        commentObj['timestamp'] = comment['data-comment-timestamp']\n",
    "        commentObj['author'] = comment['data-comment-author'].encode('utf-8')\n",
    "        commentObj['author-id'] = comment['data-comment-author-id']\n",
    "        \n",
    "        body = comment.find(class_='d-comment__body')\n",
    "\n",
    "        if body.blockquote is not None:\n",
    "            body.blockquote.clear()\n",
    "\n",
    "        commentObj['text'] = body.getText().strip().encode('utf-8')\n",
    "        \n",
    "\n",
    "        recommend = comment.find(class_='d-comment__recommend')\n",
    "\n",
    "        if recommend is not None:\n",
    "            commentObj['reccomend-count'] = recommend['data-recommend-count']\n",
    "        else:\n",
    "            commentObj['reccomend-count'] = ''\n",
    "            \n",
    "\n",
    "        replyto = comment.find(class_='d-comment__reply-to-author')\n",
    "\n",
    "        if replyto is not None:\n",
    "            link = replyto.parent['href'].replace('#comment-', '')\n",
    "            commentObj['reply-to'] = link\n",
    "        else:\n",
    "            commentObj['reply-to'] = ''\n",
    "            \n",
    "\n",
    "        comment_array.append(commentObj)\n",
    "        \n",
    "    return comment_array\n",
    "\n",
    "        \n",
    "\n",
    "url = 'https://www.theguardian.com/music/2019/feb/21/peter-tork-monkees-dies-aged-77'\n",
    "soup=get_html(url)\n",
    "comment_url = soup.find(class_='discussion__heading').find('a')['href']\n",
    "\n",
    "comments=get_comments(comment_url)\n",
    "\n",
    "\n",
    "articletitle = soup.find('h1', class_='content__headline').getText().strip().encode('utf-8')   \n",
    "fname=articletitle+'_comments'\n",
    "save(fname,comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple pages of comments? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the article has more than one page of comments the above script will not work. This is because `comment_url` only directs to the first page of comments and therefore comments on any additional pages will be missed.\n",
    "\n",
    "The urls for additional pages of comments have the structure `comment_url` with the additional text `?page=n` added to the end. Here we use `n` to denote the page number.\n",
    "\n",
    "In order to adapt the above code to deal with articles that have multiple pages of comments we need to count the number of pages there are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "comment_soup = get_html(comment_url)\n",
    "\n",
    "pagination_btn = comment_soup.find_all('a', class_='pagination__action')\n",
    "lastpagination_btn = comment_soup.find('a', class_='pagination__action--last')\n",
    "\n",
    "if lastpagination_btn is not None:\n",
    "    total_pages = int(lastpagination_btn['data-page'])\n",
    "elif pagination_btn:\n",
    "    total_pages = int(pagination_btn[-1]['data-page'])\n",
    "else:\n",
    "    total_pages = 1\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the number of pages we find the elements that contain hyperlinks to the additional pages of comments. As they contain links they will have `a` tags therefore we can search for `a` tags with the correct `class`. \n",
    "\n",
    "Depending on the number of additional pages an article can have:\n",
    "- a button to take you straight to the last page, `lastpaignation_btn`\n",
    "- buttons to take you to specific pages, `paignation_btn` \n",
    "- only one page of comments and no buttons.\n",
    "\n",
    "Each of these situations is handled within the `if...elif...else` statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how many comment pages we have, we can extend the original script to loop through all pages and scrape all of the comments with two new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pages(soup):\n",
    "    \n",
    "    pagination_btn = soup.find_all('a', class_='pagination__action')\n",
    "    lastpagination_btn = soup.find('a', class_='pagination__action--last')\n",
    "\n",
    "    if lastpagination_btn is not None:\n",
    "        total_pages = int(lastpagination_btn['data-page'])\n",
    "    elif pagination_btn:\n",
    "        total_pages = int(pagination_btn[-1]['data-page'])\n",
    "    else:\n",
    "        total_pages = 1\n",
    "    \n",
    "    return total_pages\n",
    "\n",
    "def all_comments(soup):\n",
    "    \n",
    "    try:\n",
    "        comment_url = soup.find(class_='discussion__heading').find('a')['href']\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "        \n",
    "    comment_soup=get_html(comment_url)\n",
    "    pages=count_pages(comment_soup)\n",
    "\n",
    "    allcomments=[]\n",
    "    for n in range(1,pages+1):\n",
    "        \n",
    "        page_url = comment_url+'?='+urllib.urlencode({'page': n})\n",
    "        comments=get_comments(pageurl)\n",
    "        \n",
    "        allcomments=allcomments+comments\n",
    "\n",
    "    return allcomments\n",
    "\n",
    "\n",
    "url = 'https://www.theguardian.com/music/2019/feb/21/peter-tork-monkees-dies-aged-77'\n",
    "soup=get_html(url)\n",
    "comments=all_comments(soup)\n",
    "\n",
    "\n",
    "article_title = soup.find('h1', class_='content__headline').getText().strip().encode('utf-8')   \n",
    "fname=article_title+'_allcomments'\n",
    "save(fname,comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping comments from multiple articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used the Guardian's [API](https://open-platform.theguardian.com) to obtain article data, including urls, scraping comments from all of the articles you have obtained involves iterating over a list of the urls. For instructions on how to use the Guardian's API see our tutorial [Downloading Article Data from the Guardian API](\"Downloading_Article_Data_from_the_Guardian_API.ipynb\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have a text file with one url per line, we can again adapt the code to loop through the file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "url_fname = './Guardian-comment-scraper/urls-2018-01-01-2018-01-02.txt'\n",
    "\n",
    "url_list = [line.rstrip('\\n') for line in open(url_fname)]\n",
    "    \n",
    "for url in url_list:\n",
    "    \n",
    "    soup=get_html(url)\n",
    "    comments=all_comments(soup)\n",
    "    \n",
    "    if comments==0:\n",
    "        continue\n",
    "    output.append(comments)\n",
    "    \n",
    "fname='all_url_comments'\n",
    "save(fname,output)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that we use read the lines in the text file to a list `url_list`. We then iterate over the urls in the list using a `for` loop. For each url we scrape the comments using the functions we have used previously.\n",
    "\n",
    "**Note:** the `if` statement in our for loop is there to account for articles that dont have any comments. You will notice that in the function `all_comments()`, we have used a `try` block for finding `comment_url` which allows us to handle the error if there are no comments. The error is handled using the `except` block, causing the function to return `0`, equivalent to `None`, if the article has no comments.\n",
    "\n",
    "An easier solution is to use the `commentable` field when accessing the Guardian's API: by adding this (boolean) parameter to the parameter dictionary with a value of 1, only articles that can be commented upon will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fname = './Guardian-comment-scraper/urls-2018-01-01-2018-01-02.txt'\n",
    "\n",
    "url_list = [line.rstrip('\\n') for line in open(url_fname)]\n",
    "    \n",
    "for url in url_list:\n",
    "    \n",
    "    soup=get_html(url)\n",
    "    comments=all_comments(soup)\n",
    "    \n",
    "    if comments==0:\n",
    "        continue\n",
    "    output.append(comments)\n",
    "    \n",
    "fname='all_url_comments'\n",
    "save(fname,output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
